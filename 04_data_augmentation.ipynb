{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Data Augmentation for Deep Learning</h1>\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "1. SimpleITK supports a variety of spatial transformations (global or local) that can be used to augment your dataset via resampling directly from the original images (which vary in size).\n",
    "2. Resampling to a uniform size can be done either by specifying the desired sizes resulting in non-isotropic pixel spacings (most often) or by specifying an isotropic pixel spacing and one of the image sizes (width,height,depth). \n",
    "3. SimpleITK supports a variety of intensity transformations (blurring, adding noise etc.) that can be used to augment your dataset after it has been resampled to the size expected by your network.\n",
    "\n",
    "This notebook illustrates the use of SimpleITK to perform data augmentation for deep learning. Note that the code is written so that the relevant functions work for both 2D and 3D images without modification.\n",
    "\n",
    "Data augmentation is a model based approach for enlarging your training set. The problem being addressed is that the original dataset is not sufficiently representative of the general population of images. As a consequence, if we only train on the original dataset the resulting network will not generalize well to the population (overfitting). \n",
    "\n",
    "Using a model of the variations found in the general population and the existing dataset we generate additional images in the hope of capturing the population variability. Note that if the model you use is incorrect you can cause harm, you are generating observations that do not occur in the general population and are optimizing a function to fit them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib notebook\n",
    "import gui\n",
    "\n",
    "from downloaddata import fetch_data as fdata\n",
    "from utilities import (\n",
    "    parameter_space_regular_grid_sampling,\n",
    "    similarity3D_parameter_space_regular_sampling,\n",
    "    eul2quat,\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load the images. You can work through the notebook using either the original 3D images or 2D slices from the original volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching training_001_ct.mha\n",
      "Fetching training_001_mr_T1.mha\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    sitk.ReadImage(fdata(\"training_001_ct.mha\")),\n",
    "    sitk.ReadImage(fdata(\"training_001_mr_T1.mha\")),\n",
    "]\n",
    "# Comment out the following line if you want to work in 3D. Note that in 3D some of the notebook visualizations are\n",
    "# disabled.\n",
    "data = [data[0][:, :, data[0].GetDepth() // 2], data[1][:, :, data[1].GetDepth() // 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='fd8de43b-c1f7-4ebc-ac02-c5ee503aed48'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def disp_images(images, fig_size, wl_list=None):\n",
    "    if images[0].GetDimension() == 2:\n",
    "        gui.multi_image_display2D(\n",
    "            image_list=images, figure_size=fig_size, window_level_list=wl_list\n",
    "        )\n",
    "    else:\n",
    "        gui.MultiImageDisplay(\n",
    "            image_list=images, figure_size=fig_size, window_level_list=wl_list\n",
    "        )\n",
    "\n",
    "\n",
    "disp_images(data, fig_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data often needs to be modified. In this example we would like to crop the images so that we only keep the informative regions. We can readily separate the foreground and background using an appropriate threshold, in our case we use Otsu's threshold selection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_based_crop_and_bg_median(image):\n",
    "    \"\"\"\n",
    "    Use Otsu's threshold estimator to separate background and foreground. In medical imaging the background is\n",
    "    usually air. Then crop the image using the foreground's axis aligned bounding box and compute the background\n",
    "    median intensity.\n",
    "    Args:\n",
    "        image (SimpleITK image): An image where the anatomy and background intensities form a bi-modal distribution\n",
    "                                 (the assumption underlying Otsu's method.)\n",
    "    Return:\n",
    "        Cropped image based on foreground's axis aligned bounding box.\n",
    "        Background median intensity value.\n",
    "    \"\"\"\n",
    "    # Set pixels that are in [min_intensity,otsu_threshold] to inside_value, values above otsu_threshold are\n",
    "    # set to outside_value. The anatomy has higher intensity values than the background, so it is outside.\n",
    "    inside_value = 0\n",
    "    outside_value = 255\n",
    "    bin_image = sitk.OtsuThreshold(image, inside_value, outside_value)\n",
    "\n",
    "    # Get the median background intensity\n",
    "    label_intensity_stats_filter = sitk.LabelIntensityStatisticsImageFilter()\n",
    "    label_intensity_stats_filter.SetBackgroundValue(outside_value)\n",
    "    label_intensity_stats_filter.Execute(bin_image, image)\n",
    "    bg_median = label_intensity_stats_filter.GetMedian(inside_value)\n",
    "\n",
    "    # Get the bounding box of the anatomy\n",
    "    label_shape_filter = sitk.LabelShapeStatisticsImageFilter()\n",
    "    label_shape_filter.Execute(bin_image)\n",
    "    bounding_box = label_shape_filter.GetBoundingBox(outside_value)\n",
    "    # The bounding box's first \"dim\" entries are the starting index and last \"dim\" entries the size\n",
    "    return bg_median, sitk.RegionOfInterest(\n",
    "        image,\n",
    "        bounding_box[int(len(bounding_box) / 2) :],\n",
    "        bounding_box[0 : int(len(bounding_box) / 2)],\n",
    "    )\n",
    "\n",
    "\n",
    "bg_medians, modified_data = zip(\n",
    "    *[threshold_based_crop_and_bg_median(img) for img in data]\n",
    ")\n",
    "\n",
    "disp_images(modified_data, fig_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we select the images we want to work with, skip the following cell if you want to work with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation using spatial transformations\n",
    "\n",
    "We next illustrate the generation of images by specifying a list of transformation parameter values representing a sampling of the transformation's parameter space.\n",
    "\n",
    "The code below is agnostic to the specific transformation and it is up to the user to specify a valid list of transformation parameters (correct number of parameters and correct order). \n",
    "\n",
    "In most cases we can easily specify a regular grid in parameter space by specifying ranges of values for each of the parameters. In some cases specifying parameter values may be less intuitive (i.e. versor representation of rotation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reference domain \n",
    "\n",
    "All input images will be resampled onto the reference domain.\n",
    "\n",
    "This domain is defined by two constraints: the number of pixels per dimension and the physical size we want the reference domain to occupy. The former is associated with the computational constraints of deep learning where using a small number of pixels is desired. The later is associated with the SimpleITK concept of an image, it occupies a  region in physical space which should be large enough to encompass the object of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = data[0].GetDimension()\n",
    "\n",
    "# Physical image size corresponds to the largest physical size in the training set, or any other arbitrary size.\n",
    "reference_physical_size = np.zeros(dimension)\n",
    "for img in data:\n",
    "    reference_physical_size[:] = [\n",
    "        (sz - 1) * spc if sz * spc > mx else mx\n",
    "        for sz, spc, mx in zip(img.GetSize(), img.GetSpacing(), reference_physical_size)\n",
    "    ]\n",
    "\n",
    "# Create the reference image with a zero origin, identity direction cosine matrix and dimension\n",
    "reference_origin = np.zeros(dimension)\n",
    "reference_direction = np.identity(dimension).flatten()\n",
    "\n",
    "# Select arbitrary number of pixels per dimension, smallest size that yields desired results\n",
    "# or the required size of a pretrained network (e.g. VGG-16 224x224), transfer learning. This will\n",
    "# often result in non-isotropic pixel spacing.\n",
    "reference_size = [128] * dimension\n",
    "reference_spacing = [\n",
    "    phys_sz / (sz - 1) for sz, phys_sz in zip(reference_size, reference_physical_size)\n",
    "]\n",
    "\n",
    "# Another possibility is that you want isotropic pixels, then you can specify the image size for one of\n",
    "# the axes and the others are determined by this choice. Below we choose to set the x axis to 128 and the\n",
    "# spacing set accordingly.\n",
    "# Uncomment the following lines to use this strategy.\n",
    "# reference_size_x = 128\n",
    "# reference_spacing = [reference_physical_size[0]/(reference_size_x-1)]*dimension\n",
    "# reference_size = [int(phys_sz/(spc) + 1) for phys_sz,spc in zip(reference_physical_size, reference_spacing)]\n",
    "\n",
    "reference_image = sitk.Image(reference_size, data[0].GetPixelIDValue())\n",
    "reference_image.SetOrigin(reference_origin)\n",
    "reference_image.SetSpacing(reference_spacing)\n",
    "reference_image.SetDirection(reference_direction)\n",
    "\n",
    "# Always use the TransformContinuousIndexToPhysicalPoint to compute an indexed point's physical coordinates as\n",
    "# this takes into account size, spacing and direction cosines. For the vast majority of images the direction\n",
    "# cosines are the identity matrix, but when this isn't the case simply multiplying the central index by the\n",
    "# spacing will not yield the correct coordinates resulting in a long debugging session.\n",
    "reference_center = np.array(\n",
    "    reference_image.TransformContinuousIndexToPhysicalPoint(\n",
    "        np.array(reference_image.GetSize()) / 2.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "Once we have a reference domain we can augment the data using any of the SimpleITK global domain transformations. In this notebook we use a similarity transformation (the generate_images function is agnostic to this specific choice).\n",
    "\n",
    "Note that you also need to create the labels for your augmented images. If these are just classes then your processing is minimal. If you are dealing with segmentation you will also need to transform the segmentation labels so that they match the transformed image. The following function easily accommodates for this, just provide the labeled image as input and use the `sitk.sitkNearestNeighbor` interpolator so that you do not introduce labels that were not in the original segmentation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images_spatial(\n",
    "    original_image,\n",
    "    reference_image,\n",
    "    T0,\n",
    "    T_aug,\n",
    "    transformation_parameters,\n",
    "    output_prefix,\n",
    "    output_suffix,\n",
    "    interpolator=sitk.sitkLinear,\n",
    "    default_intensity_value=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate the resampled images based on the given transformations.\n",
    "    Args:\n",
    "        original_image (SimpleITK image): The image which we will resample and transform.\n",
    "        reference_image (SimpleITK image): The image onto which we will resample.\n",
    "        T0 (SimpleITK transform): Transformation which maps points from the reference image coordinate system\n",
    "            to the original_image coordinate system.\n",
    "        T_aug (SimpleITK transform): Map points from the reference_image coordinate system back onto itself using the\n",
    "               given transformation_parameters. The reason we use this transformation as a parameter\n",
    "               is to allow the user to set its center of rotation to something other than zero.\n",
    "        transformation_parameters (List of lists): parameter values which we use T_aug.SetParameters().\n",
    "        output_prefix (string): output file name prefix (file name: output_prefix_p1_p2_..pn_.output_suffix).\n",
    "        output_suffix (string): output file name suffix (file name: output_prefix_p1_p2_..pn_.output_suffix).\n",
    "        interpolator: One of the SimpleITK interpolators.\n",
    "        default_intensity_value: The value to return if a point is mapped outside the original_image domain.\n",
    "    \"\"\"\n",
    "    all_images = []  # Used only for display purposes in this notebook.\n",
    "    for current_parameters in transformation_parameters:\n",
    "        T_aug.SetParameters(current_parameters)\n",
    "        # Augmentation is done in the reference image space, so we first map the points from the reference image space\n",
    "        # back onto itself T_aug (e.g. rotate the reference image) and then we map to the original image space T0.\n",
    "        T_all = sitk.CompositeTransform([T0, T_aug])\n",
    "        aug_image = sitk.Resample(\n",
    "            original_image,\n",
    "            reference_image,\n",
    "            T_all,\n",
    "            interpolator,\n",
    "            default_intensity_value,\n",
    "        )\n",
    "        sitk.WriteImage(\n",
    "            aug_image,\n",
    "            output_prefix\n",
    "            + \"_\"\n",
    "            + \"_\".join(str(param) for param in current_parameters)\n",
    "            + \"_.\"\n",
    "            + output_suffix,\n",
    "        )\n",
    "\n",
    "        all_images.append(aug_image)  # Used only for display purposes in this notebook.\n",
    "    return all_images  # Used only for display purposes in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the generate_images function we need to compute the transformation which will map points between the reference image and the current image as shown in the code cell below. \n",
    "\n",
    "Note that it is very easy to generate large amounts of data, the calls to np.linspace with $m$ parameters each having $n$ values results in $n^m$ images, so don't forget that these images are also saved to disk. **If you run the code below for 3D data you will generate 4374 volumes ($3^7$ parameter combinations times 2 volumes).**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dimension == 2:\n",
    "    # The parameters are scale (+-10%), rotation angle (+-10 degrees), x translation, y translation\n",
    "    transformation_parameters_list = parameter_space_regular_grid_sampling(\n",
    "        np.linspace(0.9, 1.1, 3),\n",
    "        np.linspace(-np.pi / 18.0, np.pi / 18.0, 3),\n",
    "        np.linspace(-10, 10, 3),\n",
    "        np.linspace(-10, 10, 3),\n",
    "    )\n",
    "    aug_transform = sitk.Similarity2DTransform()\n",
    "else:\n",
    "    transformation_parameters_list = similarity3D_parameter_space_regular_sampling(\n",
    "        np.linspace(-np.pi / 18.0, np.pi / 18.0, 3),\n",
    "        np.linspace(-np.pi / 18.0, np.pi / 18.0, 3),\n",
    "        np.linspace(-np.pi / 18.0, np.pi / 18.0, 3),\n",
    "        np.linspace(-10, 10, 3),\n",
    "        np.linspace(-10, 10, 3),\n",
    "        np.linspace(-10, 10, 3),\n",
    "        np.linspace(0.9, 1.1, 3),\n",
    "    )\n",
    "    aug_transform = sitk.Similarity3DTransform()\n",
    "\n",
    "all_images = []\n",
    "for index, img in enumerate(data):\n",
    "    # Transform which maps from the reference_image to the current img with the translation mapping the image\n",
    "    # origins to each other.\n",
    "    transform = sitk.AffineTransform(dimension)\n",
    "    transform.SetMatrix(img.GetDirection())\n",
    "    transform.SetTranslation(np.array(img.GetOrigin()) - reference_origin)\n",
    "    # Modify the transformation to align the centers of the original and reference image instead of their origins.\n",
    "    centering_transform = sitk.TranslationTransform(dimension)\n",
    "    img_center = np.array(\n",
    "        img.TransformContinuousIndexToPhysicalPoint(np.array(img.GetSize()) / 2.0)\n",
    "    )\n",
    "    centering_transform.SetOffset(\n",
    "        np.array(transform.GetInverse().TransformPoint(img_center) - reference_center)\n",
    "    )\n",
    "    centered_transform = sitk.CompositeTransform(transform)\n",
    "    centered_transform.AddTransform(centering_transform)\n",
    "\n",
    "    # Set the augmenting transform's center so that rotation is around the image center.\n",
    "    aug_transform.SetCenter(reference_center)\n",
    "\n",
    "    generated_images = augment_images_spatial(\n",
    "        img,\n",
    "        reference_image,\n",
    "        centered_transform,\n",
    "        aug_transform,\n",
    "        transformation_parameters_list,\n",
    "        os.path.join(OUTPUT_DIR, \"spatial_aug\" + str(index)),\n",
    "        \"mha\",\n",
    "        default_intensity_value=bg_medians[index],\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        dimension == 2\n",
    "    ):  # in 2D we join all of the images into a 3D volume which we use for display.\n",
    "        all_images.append(sitk.JoinSeries(generated_images))\n",
    "# If working in 2D, display the resulting set of images.\n",
    "if dimension == 2:\n",
    "    gui.MultiImageDisplay(image_list=all_images, shared_slider=True, figure_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What about flipping\n",
    "\n",
    "Reflection using SimpleITK can be done in one of several ways:\n",
    "1. Use an affine transform with the matrix component set to a reflection matrix. The columns of the matrix correspond to the $\\mathbf{x}, \\mathbf{y}$ and $\\mathbf{z}$ axes. The reflection matrix is constructed using the plane, 3D,  or axis, 2D, which we want to reflect through with the standard basis vectors, $\\mathbf{e}_i, \\mathbf{e}_j$, and the remaining basis vector set to $-\\mathbf{e}_k$.  \n",
    "    * Reflection about $xy$ plane: $[\\mathbf{e}_1, \\mathbf{e}_2, -\\mathbf{e}_3]$.\n",
    "    * Reflection about $xz$ plane: $[\\mathbf{e}_1, -\\mathbf{e}_2, \\mathbf{e}_3]$.\n",
    "    * Reflection about $yz$ plane: $[-\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3]$.\n",
    "2. Use the native slicing operator(e.g. img[:,::-1,:]), or the FlipImageFilter after the image is resampled onto the reference image grid. \n",
    "\n",
    "We prefer option 1 as it is computationally more efficient. It combines all transformation prior to resampling, while the other approach performs resampling onto the reference image grid followed by the reflection operation. An additional difference is that using slicing or the FlipImageFilter will also modify the image origin while the resampling approach keeps the spatial location of the reference image origin intact. This minor difference is of no concern in deep learning as the content of the images is the same, but in SimpleITK two images are considered equivalent iff their content and spatial extent are the same.\n",
    "\n",
    "The following cell corresponds to the preferred option, using an affine transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_images = []\n",
    "for img in data:\n",
    "    # Compute the transformation which maps between the reference and current image (same as done above).\n",
    "    transform = sitk.AffineTransform(dimension)\n",
    "    transform.SetMatrix(img.GetDirection())\n",
    "    transform.SetTranslation(np.array(img.GetOrigin()) - reference_origin)\n",
    "    centering_transform = sitk.TranslationTransform(dimension)\n",
    "    img_center = np.array(\n",
    "        img.TransformContinuousIndexToPhysicalPoint(np.array(img.GetSize()) / 2.0)\n",
    "    )\n",
    "    centering_transform.SetOffset(\n",
    "        np.array(transform.GetInverse().TransformPoint(img_center) - reference_center)\n",
    "    )\n",
    "    centered_transform = sitk.CompositeTransform([transform, centering_transform])\n",
    "\n",
    "    flipped_transform = sitk.AffineTransform(dimension)\n",
    "    flipped_transform.SetCenter(\n",
    "        reference_image.TransformContinuousIndexToPhysicalPoint(\n",
    "            np.array(reference_image.GetSize()) / 2.0\n",
    "        )\n",
    "    )\n",
    "    if dimension == 2:  # matrices in SimpleITK specified in row major order\n",
    "        flipped_transform.SetMatrix([1, 0, 0, -1])\n",
    "    else:\n",
    "        flipped_transform.SetMatrix([1, 0, 0, 0, -1, 0, 0, 0, 1])\n",
    "    centered_transform.AddTransform(flipped_transform)\n",
    "\n",
    "    # Resample onto the reference image\n",
    "    flipped_images.append(\n",
    "        sitk.Resample(img, reference_image, centered_transform, sitk.sitkLinear, 0.0)\n",
    "    )\n",
    "disp_images(flipped_images, fig_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Distortion\n",
    "\n",
    "Some 2D medical imaging modalities, such as endoscopic video and X-ray images acquired with C-arms using image intensifiers, exhibit radial distortion. The common model for such distortion was described by Brown [\"Close-range camera calibration\", Photogrammetric Engineering, 37(8):855â€“866, 1971]:\n",
    "$$\n",
    "\\mathbf{p}_u = \\mathbf{p}_d + (\\mathbf{p}_d-\\mathbf{p}_c)(k_1r^2 + k_2r^4 + k_3r^6 + \\ldots)\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $\\mathbf{p}_u$ is a point in the undistorted image\n",
    "* $\\mathbf{p}_d$ is a point in the distorted image\n",
    "* $\\mathbf{p}_c$ is the center of distortion\n",
    "* $r = \\|\\mathbf{p}_d-\\mathbf{p}_c\\|$\n",
    "* $k_i$ are coefficients of the radial distortion\n",
    "\n",
    "\n",
    "Using SimpleITK operators we represent this transformation using a deformation field as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_distort(\n",
    "    image, k1, k2, k3, default_intensity_value=0.0, distortion_center=None\n",
    "):\n",
    "    c = distortion_center\n",
    "    if not c:  # The default distortion center coincides with the image center\n",
    "        c = np.array(\n",
    "            image.TransformContinuousIndexToPhysicalPoint(\n",
    "                np.array(image.GetSize()) / 2.0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Compute the vector image (p_d - p_c)\n",
    "    delta_image = sitk.PhysicalPointSource(\n",
    "        sitk.sitkVectorFloat64,\n",
    "        image.GetSize(),\n",
    "        image.GetOrigin(),\n",
    "        image.GetSpacing(),\n",
    "        image.GetDirection(),\n",
    "    )\n",
    "    delta_image_list = [\n",
    "        sitk.VectorIndexSelectionCast(delta_image, i) - c[i] for i in range(len(c))\n",
    "    ]\n",
    "\n",
    "    # Compute the radial distortion expression\n",
    "    r2_image = sitk.NaryAdd([img**2 for img in delta_image_list])\n",
    "    r4_image = r2_image**2\n",
    "    r6_image = r2_image * r4_image\n",
    "    disp_image = k1 * r2_image + k2 * r4_image + k3 * r6_image\n",
    "    displacement_image = sitk.Compose([disp_image * img for img in delta_image_list])\n",
    "\n",
    "    displacement_field_transform = sitk.DisplacementFieldTransform(displacement_image)\n",
    "    return sitk.Resample(\n",
    "        image,\n",
    "        image,\n",
    "        displacement_field_transform,\n",
    "        sitk.sitkLinear,\n",
    "        default_intensity_value,\n",
    "        image.GetPixelID(),\n",
    "    )\n",
    "\n",
    "\n",
    "k1 = 0.00001\n",
    "k2 = 0.0000000000001\n",
    "k3 = 0.0000000000001\n",
    "original_ct_image = data[0]\n",
    "air_HU = -1000  # Hounsfield Unit value for air is -1000, background intensity for CT\n",
    "\n",
    "# Using the default intensity value, zero, for background may not be the best choice when you have\n",
    "# prior knowledge (e.g. we have a reasonable idea of what the background intensity value should be).\n",
    "distorted_ct_image = radial_distort(original_ct_image, k1, k2, k3)\n",
    "# distorted_ct_image = radial_distort(original_ct_image, k1, k2, k3, default_intensity_value = air_HU)\n",
    "\n",
    "# Use a grid image to highlight the distortion.\n",
    "grid_image = sitk.GridSource(\n",
    "    outputPixelType=sitk.sitkUInt16,\n",
    "    size=original_ct_image.GetSize(),\n",
    "    sigma=[0.1] * dimension,\n",
    "    gridSpacing=[20.0] * dimension,\n",
    ")\n",
    "grid_image.CopyInformation(original_ct_image)\n",
    "distorted_grid = radial_distort(grid_image, k1, k2, k3)\n",
    "disp_images([original_ct_image, distorted_ct_image, distorted_grid], fig_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferring deformations - exercise for the interested reader\n",
    "\n",
    "Using SimpleITK we can readily transfer deformations from a spatio-temporal data set to another spatial data set to simulate temporal behavior. Case in point, using a 4D (3D+time) CT of the thorax we can estimate the respiratory motion using non-rigid registration and Free Form Deformation or displacement field transformations. We can then register a new spatial data set to the original spatial CT (non-rigidly) followed by application of the temporal deformations.\n",
    "\n",
    "Note that unlike the arbitrary spatial transformations we used for data-augmentation above this approach is more computationally expensive as it involves multiple non-rigid registrations. Also note that as the goal is to use the estimated transformations to create plausible deformations you may be able to relax the required registration accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation using intensity modifications\n",
    "\n",
    "SimpleITK has many filters that are potentially relevant for data augmentation via modification of intensities. For example:\n",
    "* Image smoothing, always read the documentation carefully, similar filters use use different parametrization $\\sigma$ vs. variance ($\\sigma^2$):\n",
    "  * [Discrete Gaussian](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1DiscreteGaussianImageFilter.html)\n",
    "  * [Recursive Gaussian](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1RecursiveGaussianImageFilter.html)\n",
    "  * [Smoothing Recursive Gaussian](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1SmoothingRecursiveGaussianImageFilter.html)\n",
    "\n",
    "* Edge preserving image smoothing:\n",
    "  * [Bilateral image filtering](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1BilateralImageFilter.html), edge preserving smoothing.\n",
    "  * [Median filtering](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1MedianImageFilter.html)\n",
    "\n",
    "* Adding noise to your images:\n",
    "  * [Additive Gaussian](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1AdditiveGaussianNoiseImageFilter.html)\n",
    "  * [Salt and Pepper / Impulse](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1SaltAndPepperNoiseImageFilter.html)\n",
    "  * [Shot/Poisson](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1ShotNoiseImageFilter.html)\n",
    "  * [Speckle/multiplicative](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1SpeckleNoiseImageFilter.html)\n",
    "  \n",
    "* [Adaptive Histogram Equalization](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1AdaptiveHistogramEqualizationImageFilter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images_intensity(image_list, output_prefix, output_suffix):\n",
    "    \"\"\"\n",
    "    Generate intensity modified images from the originals.\n",
    "    Args:\n",
    "        image_list (iterable containing SimpleITK images): The images whose intensities we modify.\n",
    "        output_prefix (string): output file name prefix (file name: output_prefixi_FilterName.output_suffix).\n",
    "        output_suffix (string): output file name suffix (file name: output_prefixi_FilterName.output_suffix).\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of intensity modifying filters, which we apply to the given images\n",
    "    filter_list = []\n",
    "\n",
    "    # Smoothing filters\n",
    "\n",
    "    filter_list.append(sitk.SmoothingRecursiveGaussianImageFilter())\n",
    "    filter_list[-1].SetSigma(2.0)\n",
    "\n",
    "    filter_list.append(sitk.DiscreteGaussianImageFilter())\n",
    "    filter_list[-1].SetVariance(4.0)\n",
    "\n",
    "    filter_list.append(sitk.BilateralImageFilter())\n",
    "    filter_list[-1].SetDomainSigma(4.0)\n",
    "    filter_list[-1].SetRangeSigma(8.0)\n",
    "\n",
    "    filter_list.append(sitk.MedianImageFilter())\n",
    "    filter_list[-1].SetRadius(8)\n",
    "\n",
    "    # Noise filters using default settings\n",
    "\n",
    "    # Filter control via SetMean, SetStandardDeviation.\n",
    "    filter_list.append(sitk.AdditiveGaussianNoiseImageFilter())\n",
    "\n",
    "    # Filter control via SetProbability\n",
    "    filter_list.append(sitk.SaltAndPepperNoiseImageFilter())\n",
    "\n",
    "    # Filter control via SetScale\n",
    "    filter_list.append(sitk.ShotNoiseImageFilter())\n",
    "\n",
    "    # Filter control via SetStandardDeviation\n",
    "    filter_list.append(sitk.SpeckleNoiseImageFilter())\n",
    "\n",
    "    filter_list.append(sitk.AdaptiveHistogramEqualizationImageFilter())\n",
    "    filter_list[-1].SetAlpha(1.0)\n",
    "    filter_list[-1].SetBeta(0.0)\n",
    "\n",
    "    filter_list.append(sitk.AdaptiveHistogramEqualizationImageFilter())\n",
    "    filter_list[-1].SetAlpha(0.0)\n",
    "    filter_list[-1].SetBeta(1.0)\n",
    "\n",
    "    aug_image_lists = []  # Used only for display purposes in this notebook.\n",
    "    for i, img in enumerate(image_list):\n",
    "        aug_image_lists.append([f.Execute(img) for f in filter_list])\n",
    "        for aug_image, f in zip(aug_image_lists[-1], filter_list):\n",
    "            sitk.WriteImage(\n",
    "                aug_image,\n",
    "                output_prefix + str(i) + \"_\" + f.GetName() + \".\" + output_suffix,\n",
    "            )\n",
    "    return aug_image_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the intensities of the original images using the set of SimpleITK filters described above. If we are working with 2D images the results will be displayed inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_augmented_images = augment_images_intensity(\n",
    "    data, os.path.join(OUTPUT_DIR, \"intensity_aug\"), \"mha\"\n",
    ")\n",
    "\n",
    "# in 2D we join all of the images into a 3D volume which we use for display.\n",
    "if dimension == 2:\n",
    "\n",
    "    def list2_float_volume(image_list):\n",
    "        return sitk.JoinSeries([sitk.Cast(img, sitk.sitkFloat32) for img in image_list])\n",
    "\n",
    "    all_images = [list2_float_volume(imgs) for imgs in intensity_augmented_images]\n",
    "\n",
    "    # Compute reasonable window-level values for display (just use the range of intensity values\n",
    "    # from the original data).\n",
    "    original_window_level = []\n",
    "    statistics_image_filter = sitk.StatisticsImageFilter()\n",
    "    for img in data:\n",
    "        statistics_image_filter.Execute(img)\n",
    "        max_intensity = statistics_image_filter.GetMaximum()\n",
    "        min_intensity = statistics_image_filter.GetMinimum()\n",
    "        original_window_level.append(\n",
    "            (max_intensity - min_intensity, (max_intensity + min_intensity) / 2.0)\n",
    "        )\n",
    "    gui.MultiImageDisplay(\n",
    "        image_list=all_images,\n",
    "        shared_slider=True,\n",
    "        figure_size=(6, 2),\n",
    "        window_level_list=original_window_level,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleITK has a sigmoid filter that allows us to map intensities via this nonlinear function to our desired range. Unlike the standard sigmoid settings that are applied when used as an activation function, the sigmoid filter is not necessarily centered on zero and the minimum and maximum output values are not necessarily 0 and 1.\n",
    "The filter itself is defined as:\n",
    "$$f(I) = (max_{output} - min_{output}) \\frac{1}{1+ e^{-\\frac{I-\\beta}{\\alpha}}} + min_{output}$$\n",
    "\n",
    "Where $\\alpha$ is the curve steepness (the larger the $\\alpha$ the steeper the slope, the smaller the $\\alpha$ the closer we get to a linear mapping in the output range) and $\\beta$ is the intensity value for the sigmoid midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_mapping(\n",
    "    image, curve_steepness, output_min=0, output_max=1.0, intensity_midpoint=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Map the image using a sigmoid function.\n",
    "    Args:\n",
    "        image (SimpleITK image): scalar input image.\n",
    "        curve_steepness: Control the sigmoid steepness, the larger the number the steeper the curve.\n",
    "        output_min: Minimum value for output image, default 0.0 .\n",
    "        output_max: Maximum value for output image, default 1.0 .\n",
    "        intensity_midpoint: intensity value defining the sigmoid midpoint (x coordinate), default is the\n",
    "                            median image intensity.\n",
    "    Return:\n",
    "        SimpleITK image with float pixel type.\n",
    "    \"\"\"\n",
    "    if intensity_midpoint is None:\n",
    "        intensity_midpoint = np.median(sitk.GetArrayViewFromImage(image))\n",
    "\n",
    "    sig_filter = sitk.SigmoidImageFilter()\n",
    "    sig_filter.SetOutputMinimum(output_min)\n",
    "    sig_filter.SetOutputMaximum(output_max)\n",
    "    sig_filter.SetAlpha(1.0 / curve_steepness)\n",
    "    sig_filter.SetBeta(float(intensity_midpoint))\n",
    "    return sig_filter.Execute(sitk.Cast(image, sitk.sitkFloat64))\n",
    "\n",
    "\n",
    "# Change the order of magnitude of curve steepness [1.0,0.1,0.01] to see the effect of this parameter.\n",
    "# Also change it from positive to negative.\n",
    "disp_images(\n",
    "    [sigmoid_mapping(img, curve_steepness=0.01) for img in data], fig_size=(6, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram equalization, increasing the entropy, of images prior to using deep learning is a common preprocessing step. Unfortunately, ITK and consequentially SimpleITK do not provide the classical histogram equalization filter.\n",
    "\n",
    "In the following cell we implement this functionality for all integer scalar SimpleITK images (2D,3D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(\n",
    "    image, min_target_range=None, max_target_range=None, use_target_range=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Histogram equalization of scalar images whose single channel has an integer\n",
    "    type. The goal is to map the original intensities so that resulting\n",
    "    histogram is more uniform (increasing the image's entropy).\n",
    "    Args:\n",
    "        image (SimpleITK.Image): A SimpleITK scalar image whose pixel type\n",
    "                                 is an integer (sitkUInt8,sitkInt8...\n",
    "                                 sitkUInt64, sitkInt64).\n",
    "        min_target_range (scalar): Minimal value for the target range. If None\n",
    "                                   then use the minimal value for the scalar pixel\n",
    "                                   type (e.g. 0 for sitkUInt8).\n",
    "        max_target_range (scalar): Maximal value for the target range. If None\n",
    "                                   then use the maximal value for the scalar pixel\n",
    "                                   type (e.g. 255 for sitkUInt8).\n",
    "        use_target_range (bool): If true, the resulting image has values in the\n",
    "                                 target range, otherwise the resulting values\n",
    "                                 are in [0,1].\n",
    "    Returns:\n",
    "        SimpleITK.Image: A scalar image with the same pixel type as the input image\n",
    "                         or a sitkFloat64 (depending on the use_target_range value).\n",
    "    \"\"\"\n",
    "    arr = sitk.GetArrayViewFromImage(image)\n",
    "\n",
    "    i_info = np.iinfo(arr.dtype)\n",
    "    if min_target_range is None:\n",
    "        min_target_range = i_info.min\n",
    "    else:\n",
    "        min_target_range = np.max([i_info.min, min_target_range])\n",
    "    if max_target_range is None:\n",
    "        max_target_range = i_info.max\n",
    "    else:\n",
    "        max_target_range = np.min([i_info.max, max_target_range])\n",
    "\n",
    "    min_val = arr.min()\n",
    "    number_of_bins = arr.max() - min_val + 1\n",
    "    # using ravel, not flatten, as it does not involve memory copy\n",
    "    hist = np.bincount((arr - min_val).ravel(), minlength=number_of_bins)\n",
    "    cdf = np.cumsum(hist)\n",
    "    cdf = (cdf - cdf[0]) / (cdf[-1] - cdf[0])\n",
    "    res = cdf[arr - min_val]\n",
    "    if use_target_range:\n",
    "        res = (min_target_range + res * (max_target_range - min_target_range)).astype(\n",
    "            arr.dtype\n",
    "        )\n",
    "    return sitk.GetImageFromArray(res)\n",
    "\n",
    "\n",
    "# cast the images to int16 because data[0] is float32 and the histogram equalization only works\n",
    "# on integer types.\n",
    "disp_images([histogram_equalization(img) for img in data], fig_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can easily create intensity variations that are specific to your domain, such as the spatially varying multiplicative and additive transformation shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_and_add_intensity_fields(original_image):\n",
    "    \"\"\"\n",
    "    Modify the intensities using multiplicative and additive Gaussian bias fields.\n",
    "    \"\"\"\n",
    "    # Gaussian image with same meta-information as original (size, spacing, direction cosine)\n",
    "    # Sigma is half the image's physical size and mean is the center of the image.\n",
    "    g_mult = sitk.GaussianSource(\n",
    "        original_image.GetPixelIDValue(),\n",
    "        original_image.GetSize(),\n",
    "        [\n",
    "            (sz - 1) * spc / 2.0\n",
    "            for sz, spc in zip(original_image.GetSize(), original_image.GetSpacing())\n",
    "        ],\n",
    "        original_image.TransformContinuousIndexToPhysicalPoint(\n",
    "            np.array(original_image.GetSize()) / 2.0\n",
    "        ),\n",
    "        25,\n",
    "        original_image.GetOrigin(),\n",
    "        original_image.GetSpacing(),\n",
    "        original_image.GetDirection(),\n",
    "    )\n",
    "\n",
    "    # Gaussian image with same meta-information as original (size, spacing, direction cosine)\n",
    "    # Sigma is 1/8 the image's physical size and mean is at 1/16 of the size\n",
    "    g_add = sitk.GaussianSource(\n",
    "        original_image.GetPixelIDValue(),\n",
    "        original_image.GetSize(),\n",
    "        [\n",
    "            (sz - 1) * spc / 8.0\n",
    "            for sz, spc in zip(original_image.GetSize(), original_image.GetSpacing())\n",
    "        ],\n",
    "        original_image.TransformContinuousIndexToPhysicalPoint(\n",
    "            np.array(original_image.GetSize()) / 16.0\n",
    "        ),\n",
    "        25,\n",
    "        original_image.GetOrigin(),\n",
    "        original_image.GetSpacing(),\n",
    "        original_image.GetDirection(),\n",
    "    )\n",
    "\n",
    "    return g_mult * original_image + g_add\n",
    "\n",
    "\n",
    "disp_images([mult_and_add_intensity_fields(img) for img in data], fig_size=(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"05_basic_registration.ipynb\"><h2 align=right>Next &raquo;</h2></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
